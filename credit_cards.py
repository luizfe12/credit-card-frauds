# -*- coding: utf-8 -*-
"""Credit-cards.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kZt4ZocsWUWIee7pdPEWCCSjcbWE0vR3
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
#importando banco de dados
BD = pd.read_csv("creditcard.csv")
#descobrindo a proporçao de fraudes
fraudes = len(BD[BD['Class'] == 1])
proporcao = fraudes / len(BD) * 100
print(proporcao)

#plotando um grafico para comparar fraude x nao fraude apartir de Amount
sns.boxplot(x='Class', y='Amount', data=BD)
plt.title('Distribuição dos Valores de Transação por Classe', fontsize=16)
plt.xlabel('Classe', fontsize=12)
plt.ylabel('Valor da Transação (Amount)', fontsize=12)
plt.show()

#analise dos dados
print(BD)
#apos a análise notei que os dados já estão muito bem pré processados, todas numéricas
#porem falta padronizar algumas coisas ainda, como as colunas Time e Amount
#pois todas as outras vao em media de -5 a 5, já time e amount são bem maiores

#primeiro verificamos se há algum dado faltante
print(BD.isnull().sum())

#agora vamos
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# Aplicar o scaler APENAS nas colunas 'Time' e 'Amount'
BD['scaled_amount'] = scaler.fit_transform(BD['Amount'].values.reshape(-1, 1))
BD['scaled_time'] = scaler.fit_transform(BD['Time'].values.reshape(-1, 1))

# Agora, podemos remover as colunas originais
BD = BD.drop(['Time', 'Amount'], axis=1)
print(BD)

#agora separamos as colunas que servirao como 'alvo' e 'features'
X = BD.drop('Class', axis = 1)
y = BD['Class']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12, stratify=y)

# AGORA, VAMOS APLICAR O SMOTE
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=12)

# Aplicando o SMOTE APENAS nos dados de treino
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

#agora meus dados estão prontos para começar o treinamento

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Usamos os dados de treino que foram balanceados com o SMOTE
model = LogisticRegression(random_state=12, )
model.fit(X_train_resampled, y_train_resampled)

# 2. Fazer previsões com os dados de teste

y_pred = model.predict(X_test)

# 3. Avaliar a performance do modelo
# A Matriz de Confusão mostra exatamente onde o modelo acertou e errou
print("\n--- Matriz de Confusão (Regressão Logística) ---")
cm = confusion_matrix(y_test, y_pred)
print(cm)
print("\n--- Classification Report (Regressão Logística) ---")
print(classification_report(y_test, y_pred))


#Agora vamos testar com o RandomForest

from sklearn.ensemble import RandomForestClassifier
model_rf = RandomForestClassifier(n_estimators=100, random_state=12, n_jobs=-1)
model_rf.fit(X_train_resampled, y_train_resampled)

y_pred_rf = model_rf.predict(X_test)

print("\n--- Classification Report (Random Forest) ---")
print(classification_report(y_test, y_pred_rf))
cm_rf= confusion_matrix(y_test, y_pred_rf)
print(cm_rf)


#Por ultimo avaliamos o xgb
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Vamos usar alguns parâmetros iniciais comuns.
model_xgb = XGBClassifier(
    n_estimators=1000,      # Número de "rodadas" de boosting. Um valor alto é comum.
    learning_rate=0.05,      # Taxa de aprendizado. Valores baixos são geralmente melhores.
    use_label_encoder=False,# Para evitar um aviso de depreciação.
    eval_metric='logloss',  # Métrica de avaliação interna.
    n_jobs=-1,              # Usa todos os processadores para acelerar.
    random_state=42
)

model_xgb.fit(X_train_resampled, y_train_resampled)
previsoes_xgb = model_xgb.predict(X_test)

print("\n--- Matriz de Confusão (XGBoost) ---")
cm_xgb = confusion_matrix(y_test, previsoes_xgb)
print(cm_xgb)
print("\n--- Classification Report (XGBoost) ---")
print(classification_report(y_test, previsoes_xgb))